{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабараторная работа 1. MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На библиотеке \"Keras\" реализовать MLP.\n",
    "\n",
    "В качестве примера взять простой набор данных из Sklearn:\n",
    "http://scikit-learn.org/stable/datasets/#sample-generators\n",
    "\n",
    "MLP сделать из двух слоёв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Подключаем нужнае библиотеки:\n",
    "import numpy as np\n",
    "\n",
    "# Библиотеки sklearn, для данных, их подготовки, нормализации\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "# Графики\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Модель нейронной сети\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.callbacks import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Устанавливаем количество примеров\n",
    "n_samples = 1500\n",
    "\n",
    "# Загружаем набор данных из sklearn\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
    "x, y = noisy_circles\n",
    "\n",
    "# Нормализуем данные:\n",
    "# Преобразует данные в значения от 0 до 1\n",
    "X = MinMaxScaler().fit_transform(x)\n",
    "\n",
    "# Преобразует список результатов в бинарную матрицу\n",
    "# [0, 1] - > [[1. 0.], \n",
    "#             [0. 1.]]\n",
    "Y = np_utils.to_categorical(y, 2)\n",
    "\n",
    "#Делим загруженные данные на обучающую и тестировочную выборки\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Визуализация исходного множества:\n",
    "plt.figure()\n",
    "plt.title(\"Original space\")\n",
    "reds = y == 0\n",
    "blues = y == 1\n",
    "\n",
    "plt.plot(x[reds, 0], x[reds, 1], \"ro\")\n",
    "plt.plot(x[blues, 0], x[blues, 1], \"bo\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Создаём последовательную модель, которая представляет собой линейный стек слоев.\n",
    "model = Sequential()\n",
    "\n",
    "# Добавляем входной слой, с 4 нейронами (= число выходных сигналов) и числом входных сигналов (у нас это две координаты точек) \n",
    "model.add(Dense(4, input_shape=train_x.shape[1:]))\n",
    "# Устанавливаем функцию активации - гиперболический тангенс: 'tanh'.\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "#Добавляем ещё один скрытый слой c 2 нейронами.\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "# Добавляем выходной слой. Он состоит из 2 нейронов, потому что у нас 2 выходных значения\n",
    "model.add(Dense(2))\n",
    "# Применяем Функцию активации 'softmax'\n",
    "# Эта функция активации помогает трактовать выходной слой, как вероятность событий, так как сумма выходов слоя равняется единице.\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Настраиваем процесс обучения с одной из функций оптимизации и функций ошибки. Устанавливаем рассматриваемое значение метрики - точность(=['accuracy'])\n",
    "# Функция оптимизации: 'SGD' c leaning rate: 0.1 (lr=0.1)\n",
    "# Функция ошибки: 'mse'\n",
    "sgd = SGD(lr=0.1, decay=0.0, momentum=0.0)\n",
    "model.compile(loss='mse', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#Параметры ранней остановки и сохранение лучшей модели\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=90)\n",
    "callback_save_model = ModelCheckpoint(\"models/Lab1.hdf5\", save_best_only=True)\n",
    "\n",
    "# Обучаем построенную модель на 150 эпохах и batch_size=10 с validation_split=0.1\n",
    "history = model.fit(train_x, train_y, nb_epoch=150, validation_split = 0.1, batch_size=10, callbacks = [callback_save_model, early_stopper])\n",
    "# Проверяем модель на тестировочной выборке с verbose=0\n",
    "# параметр verbose - отвечает за протоколирование процесса. 0 - нет протоколирования, 1 - есть.\n",
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "\n",
    "#Выводим на экран получившуюся точность нашей модели\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Схема нейрона:\n",
    "<img src=\"http://www.5byte.ru/book/1/images/ris11.gif\">\n",
    "F(S) - функция активации.\n",
    "\n",
    "\n",
    "# Функция активации: Гиперболический тангенс.\n",
    "<img src=\"https://hsto.org/files/c71/db2/a75/c71db2a756494e5298ed1d5b5f15cbc9.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Функция активации f(x), гиперболический тангенс:\n",
    "import math\n",
    "def tanhen(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append((math.exp(2*item)-1)/(math.exp(2*item)+1))\n",
    "    return a\n",
    "\n",
    "x = np.arange(-10., 10., 0.2)\n",
    "sig = tanhen(x)\n",
    "plt.plot(x,sig)\n",
    "plt.title(\"f(x): tanh\", fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция ошибки: MSE (Mean Squared Error) - Среднеквадратическая ошибка\n",
    "\n",
    "За каждую эпоху, мы считаем ошибку, отняв от идеального ответа, полученный. Далее, возводим в квадрат, после чего полученное число делим на количество эпох.\n",
    "    \n",
    "   /* Примечание: \n",
    "       Ошибка может считаться каждую эпоху. В частности, keras считает ошибку каждую эпоху, а также каждый batch_size. */\n",
    "\n",
    "$MSE = \\frac{(i_1-a_1)^2+(i_2-a_2)^2+...+(i_n-a_n)^2}{n}$, где: \n",
    "\n",
    "i - нужное значение,\n",
    "\n",
    "a - полученное значение,\n",
    "\n",
    "n - количество эпох.\n",
    "\n",
    "# Оптимизационная функция SGD (Stochastic Gradient Descent) - Стохастический градиентный спуск\n",
    "\n",
    "Эта функция позволяет менять веса нейронов. \n",
    "Обратное распространение ошибки не вносит в работу сети оптимизацию. Оно перемещает неверную информацию с конца сети на все веса внутри, чтобы другой алгоритм (в нашем случае SGD) уже смог оптимизировать эти веса так, чтобы они соответствовали нашим данным.\n",
    "\n",
    "Корректировка коэффициентов сети производится после каждого элемента обучения, где значение градиента аппроксимируются градиентом функции стоимости, вычисленном только на одном элементе обучения.\n",
    "\n",
    "$Q(w) = \\sum_{i=1}^l L(a(x_i, w), y_i)-> min(w)$, где\n",
    "\n",
    "$L(a,y)$ заданная функция ошибки.\n",
    "\n",
    "На каждой итерации алгоритма вектор w изменяется в направлении наибольшего убывания функционала Q (то есть в направлении антиградиента):\n",
    "\n",
    "$w := w-\\eta \\nabla Q(w)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Визуализация 2-ух кривых ошибок:\n",
    "plt.figure()\n",
    "plt.title(\"Training loss and validation loss\")\n",
    "plt.plot(history.history[\"loss\"], label = 'train')\n",
    "plt.plot(history.history[\"val_loss\"], label = 'valid')\n",
    "plt.xlabel(\"$Epoch$\")\n",
    "plt.ylabel(\"$Loss$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
